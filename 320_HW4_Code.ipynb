{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNvkapSLKXyX3lDvp0c1HrZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGcpZYaw9y_o"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "t4JSELTM-887"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load data\n",
        "def populate_df(slug, name):\n",
        "  df = pd.DataFrame(columns=['review', 'rating'])\n",
        "  # Get the JSON from Planet Terp\n",
        "  url = \"https://planetterp.com/api/v1/professor\"\n",
        "  response = requests.get(url, params={\"slug\": slug, \"name\": name, \"reviews\": \"true\"})\n",
        "\n",
        "  print(\"Status Code:\", response.status_code)\n",
        "\n",
        "  json = response.json()\n",
        "\n",
        "  # Extract text and numeric rating from the JSON\n",
        "  length = len(json['reviews'])\n",
        "\n",
        "  ####### TEMP BLOCK SO MY CODE DOESN'T RUN FOREVER WHILE I'M TESTING AS I WRITE: SHOULD BE TAKEN OUT FOR THE FINAL RUN #######\n",
        "  # if length > 50:\n",
        "  #   length = 50\n",
        "  ####### If I forgot to take this out, then uhhhhhhh oopsie daisies                                                    #######\n",
        "\n",
        "  for i in range(length):\n",
        "    # Fill it out in reverse order so that the most recent ones show up at the beginning, so that I could more easily validate that this is working properly\n",
        "    df.loc[i] = [json['reviews'][length - (i + 1)]['review'], json['reviews'][length - (i + 1)]['rating']]\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "NlhZKDHcU52r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate dataframes\n",
        "\n",
        "# The \"good\" professors*\n",
        "df_nelson = populate_df(\"padua-perez\", \"Nelson Padua-Perez\")\n",
        "\n",
        "df_justin = populate_df(\"wyss-gallifent\", \"Justin Wyss-Gallifent\")\n",
        "\n",
        "# \"mid\" professor*\n",
        "df_hatice = populate_df(\"sahinoglu\", \"Hatice Sahinoglu\")\n",
        "\n",
        "# The \"bad\" professors*\n",
        "df_mignerey = populate_df(\"mignerey\", \"Alice Mignerey\")\n",
        "\n",
        "df_raluca = populate_df(\"rosca\", \"Raluca Rosca\")\n",
        "\n",
        "# More professors with... \"balanced\" reviews to even out the class imbalance, hopefully\n",
        "df_cukier = populate_df(\"cukier\", \"Michel Cukier\")\n",
        "\n",
        "df_yoon = populate_df(\"yoon_ilchul\", \"Ilchul Yoon\")\n",
        "\n",
        "df_kruskal = populate_df(\"kruskal\", \"Clyde Kruskal\")\n",
        "\n",
        "df_kleiss = populate_df(\"kleiss\", \"Michael Kleiss\")\n",
        "\n",
        "df_li = populate_df(\"li_liyi\", \"Liyi Li\")\n",
        "\n",
        "df_salako = populate_df(\"salako\", \"Olubukola Salako\")\n",
        "\n",
        "df_pilachowski = populate_df(\"pilachowski\", \"Timothy Pilachowski\")\n",
        "\n",
        "# More professors to fill in the 2/3/4 star area\n",
        "\n",
        "df_cliff = populate_df(\"bakalian\", \"Cliff Bakalian\")\n",
        "\n",
        "df_koppel = populate_df(\"koppel\", \"Monique Koppel\")\n",
        "\n",
        "df_baxter = populate_df(\"baxter_ashley\", \"Ashley Baxter\")\n",
        "\n",
        "df_rainbolt = populate_df(\"rainbolt_james\", \"James Rainbolt\")\n",
        "\n",
        "df_guest = populate_df(\"guest_christiana\", \"Christiana Guest\")\n",
        "\n",
        "df_fernandes = populate_df(\"fernandes\", \"Jonathan Fernandes\")\n",
        "\n",
        "df_herman = populate_df(\"herman_larry\", \"Larry Herman\")\n",
        "\n",
        "df_total = pd.concat([df_nelson, df_justin, df_hatice, df_mignerey, df_raluca,\n",
        "                      df_cukier, df_yoon, df_kruskal, df_kleiss, df_li, df_salako, df_pilachowski,\n",
        "                      df_cliff, df_koppel, df_baxter, df_rainbolt, df_guest, df_fernandes, df_herman\n",
        "                      ], ignore_index=True)\n",
        "\n",
        "# *Note: no statistical analysis was done to back up the claims on the quality of these professors."
      ],
      "metadata": {
        "id": "lwJzhj0NVNhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data exploration\n",
        "\n",
        "# Class imbalance\n",
        "print(df_total['rating'].value_counts()/len(df_total))\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Max tokens: 1527\n",
        "print(df_total['review'].apply(lambda x: len(tokenizer.encode(x))).describe())\n",
        "# Find # of responses that exceed max token count of 512\n",
        "token_counts = df_total['review'].apply(lambda x: len(tokenizer.encode(x)))\n",
        "percent_above_512 = (token_counts > 512).mean() * 100\n",
        "print(f\"Percentage of reviews longer than 512 tokens: {percent_above_512:.2f}%\")"
      ],
      "metadata": {
        "id": "XEhebh8yRq9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning\n",
        "# Since they make up a small part of the data and increasing the token count would slow down computation significantly, just throw them out\n",
        "df_total = df_total[df_total['review'].apply(lambda x: len(tokenizer.encode(x))) <= 512]"
      ],
      "metadata": {
        "id": "UpAMfyi-_XAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code taken from the following:\n",
        "# https://medium.com/@prabhatzade/freezing-layers-and-fine-tuning-transformer-models-in-pytorch-a-simple-guide-119cad0980c6\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\", # uses GPU if available, CPU if not\n",
        "                                             dtype=torch.float32 # safest, works everywhere\n",
        "                                            )\n",
        "print(model)\n",
        "# Convert the df to a dataset for use in the trainer\n",
        "dataset = Dataset.from_pandas(df_total)\n",
        "# print(dataset)\n",
        "\n",
        "# Test train split via hugging face method\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "df_test = test_dataset.to_pandas()\n",
        "\n",
        "# reformat data as a prompt for the trainer\n",
        "def create_prompt(row, training):\n",
        "  # Append true label to prompt in training data so the transformer can learn to predict the numerical rating token, but don't target leak\n",
        "  if training:\n",
        "    row['text'] = (\n",
        "              \"<|im_start|>system\\n\"\n",
        "              \"You output a single digit 1–5. No words. No explanation.\\n\"\n",
        "              \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>user\\n\"\n",
        "              f\"{row['review']}\\nHow many stars, from 1-5, did this review give?\\n\"\n",
        "              \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>assistant\\n\"\n",
        "              f\"{row['rating']}\\n\"\n",
        "              \"<|im_end|>\"\n",
        "    )\n",
        "  else:\n",
        "    row['text'] = (\n",
        "              \"<|im_start|>system\\n\"\n",
        "              \"You output a single digit 1–5. No words. No explanation.\\n\"\n",
        "              \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>user\\n\"\n",
        "              f\"{row['review']}\\nHow many stars, from 1-5, did this review give?\\n\"\n",
        "              \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>assistant\\n\"\n",
        "    )\n",
        "  return row\n",
        "\n",
        "# tokenize data\n",
        "def tokenize(batch):\n",
        "    tokenized = tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    labels = []\n",
        "\n",
        "    # Mask padding tokens to be ignored by accuracy evaluator\n",
        "    # tokenized[\"labels\"] = []\n",
        "    # for ids in tokenized[\"input_ids\"]:\n",
        "    #   input_tokens = []\n",
        "    #   for token in ids:\n",
        "    #     if token != tokenizer.pad_token_id:\n",
        "    #       input_tokens.append(token)\n",
        "    #     else:\n",
        "    #       input_tokens.append(-100)\n",
        "\n",
        "    # Mask everything except the star prediction to be ignored by the accuracy evaluator\n",
        "    # I didn't rigorously test this with CV given the time taken to train a model, but I noticed no improvement in accuracy compared to the above approach\n",
        "    for ids in input_ids:\n",
        "      ids_copy = ids.copy()\n",
        "      label_ids = [-100] * len(ids)  # mask everything by default\n",
        "\n",
        "      # Find where the assistant answer starts\n",
        "      assistant_tokens = tokenizer.encode(\"<|im_start|>assistant\", add_special_tokens=False)\n",
        "      end_token = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)\n",
        "\n",
        "      # Locate the start of assistant\n",
        "      start_index = None\n",
        "      for i in range(len(ids_copy) - len(assistant_tokens) + 1):\n",
        "        if ids_copy[i:i+len(assistant_tokens)] == assistant_tokens:\n",
        "          start_index = i + len(assistant_tokens)\n",
        "          break\n",
        "\n",
        "      if start_index is not None:\n",
        "        # Locate the end of assistant answer\n",
        "        end_index = None\n",
        "        for i in range(start_index, len(ids_copy) - len(end_token) + 1):\n",
        "          if ids_copy[i:i+len(end_token)] == end_token:\n",
        "            end_index = i\n",
        "            break\n",
        "        if end_index is None:\n",
        "          end_index = len(ids_copy)\n",
        "\n",
        "        # Unmask only the assistant answer tokens\n",
        "        for i in range(start_index, end_index):\n",
        "          if ids_copy[i] != tokenizer.pad_token_id:\n",
        "            label_ids[i] = ids_copy[i]\n",
        "\n",
        "      labels.append(label_ids)\n",
        "    tokenized[\"labels\"] = labels\n",
        "\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Train-test split\n",
        "train_dataset = train_dataset.map(lambda row: create_prompt(row, training=True))\n",
        "test_dataset = test_dataset.map(lambda row: create_prompt(row, training=False))\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "train_dataset.set_format(\"torch\")\n",
        "test_dataset.set_format(\"torch\")\n",
        "\n",
        "# Freeze the first few layers of the encoder to avoid overfitting\n",
        "for layer in model.model.layers[:6]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Unfreeze other layers to allow fine-tuning\n",
        "for layer in model.model.layers[6:]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4, # Effectively have 4x batch size\n",
        "    num_train_epochs=4,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Define a Trainer with frozen layers\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./results\")\n",
        "tokenizer.save_pretrained(\"./results\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./results\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
      ],
      "metadata": {
        "id": "8z1iw3jLW9Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some code courtesy of Maksym Morawski\n",
        "# Predictions\n",
        "def predict_review(review):\n",
        "  prompt = (\"<|im_start|>system\\n\"\n",
        "              \"You output a single digit 1–5. No words. No explanation.\\n\"\n",
        "              \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>user\\n\"\n",
        "              f\"{review}\\nHow many stars, from 1-5, did this review give?\\n\"\n",
        "              \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>assistant\\n\"\n",
        "  )\n",
        "\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output_tokens = model.generate(**inputs,\n",
        "                                   max_new_tokens=1, do_sample=False, # set this to false?\n",
        "                                  )\n",
        "\n",
        "  return tokenizer.decode(output_tokens[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "acc = 0\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(df_test)):\n",
        "  review = df_test.loc[i]\n",
        "  prediction = int(predict_review(review['review']))\n",
        "  predictions.append(prediction)\n",
        "  if prediction == review['rating']:\n",
        "    acc += 1\n",
        "  # print(int(prediction), review['rating'])\n",
        "\n",
        "acc /= len(df_test)\n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "VYpd2nbHikp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(df_test['rating'], predictions, normalize='all')\n",
        "plt.figure(figsize=(6, 5), facecolor = \"none\")\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.stretch'] = 'condensed'\n",
        "plt.rcParams['font.weight'] = 'bold'\n",
        "fontcol = \"#000000\"\n",
        "\n",
        "plt.rcParams['text.color'] = fontcol\n",
        "plt.rcParams['axes.labelcolor'] = fontcol\n",
        "plt.rcParams['xtick.color'] = fontcol\n",
        "plt.rcParams['ytick.color'] = fontcol\n",
        "# ax.tick_params(colors=fontcol)\n",
        "# ax.xaxis.label.set_color(fontcol)\n",
        "# ax.yaxis.label.set_color(fontcol)\n",
        "# ax.title.set_color(fontcol)\n",
        "ax = sns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"YlOrBr\", cbar=True,\n",
        "            xticklabels=['1', '2', '3', '4', '5'], yticklabels=['1', '2', '3', '4', '5'])\n",
        "\n",
        "\n",
        "label_font = {\n",
        "    'family': 'DejaVu Sans',   # bundled with matplotlib\n",
        "    'weight': 'bold',\n",
        "    'stretch': 'condensed',    # makes it narrower (closest to Anton look)\n",
        "    'size': 14\n",
        "}\n",
        "\n",
        "plt.xlabel(\"Predicted Stars\", fontdict=label_font)\n",
        "plt.ylabel(\"Actual Stars\", fontdict=label_font)\n",
        "plt.title(\"Confusion Matrix (%)\", fontdict=label_font)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wa6UMISTSpc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}